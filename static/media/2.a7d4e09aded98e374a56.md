I've recently been working on *Astrarium*, a cross-platform mobile astronomy tool. One of the core features of the app is a sky viewer that allows the user to observe magnified versions of celestial objects through their phone. Prioritizing intuitive UX, we decided to implement this using AR, displaying the region of the sky which would be visible through the phone's camera.

There should be a relatively simple library to do this... right? Unfortunately, the most popular AR libraries are quite heavy and come with a boatload of dependencies. Even worse, this makes them outright incompatible with frameworks like RN+Expo, which was the case with Astrarium.

Surprisingly, I couldn't find a single existing implementation of this online. **Thus, I present here an implementation of AR-like camera controls[^1] for React Three Fiber from raw device rotation measurements.** Though I use measurements taken from the `expo-sensors` `DeviceMotion` API, similar calculations should be possible with any sensor scheme.

Before we implement controls for a camera, it is important to understand some basics about cameras in 3D space. Essentially, a camera in graphics is uniquely defined by a frustum which defines the boundaries of what objects in space are rendered to the 2D viewport. Though there are many parameters involved in building this frustum, three are relevant to our objective. One, we need to know where we are positioned in space, which we call the `eye` vector. For the purposes of this tutorial, we will stand at $(0, 0, 0)$. Next, we need to know where the camera is pointing; this is known as the `lookAt` vector. Finally, we also need a `up` vector, which specifies the direction the top of the phone is pointing[^2].

![Viewing frustum](../assets/22.png "Camera viewing frustum")

Expo's `DeviceMotion` provides a `rotation` interface which continuously samples three values: `alpha`$=\alpha$, `beta`$=\beta$, and `gamma`$=\gamma$. Although the documentation doesn't tell us what any of these mean[^3], we can easily deduce this with a little bit of experimentation. Let's first subscribe to the sensor so that we can listen to its measurements:

```ts
import { DeviceMotion } from 'expo-sensors';

const [{ alpha, beta, gamma }, setData] = useState({
    alpha: 0,
    beta: 0,
    gamma: 0,
});

const _subscribe = () => {
    DeviceMotion.addListener((deviceMotionData) => {
        setData(deviceMotionData.rotation);
    });
};

const _unsubscribe = () => {
    DeviceMotion.removeAllListeners();
};

useEffect(() => {
    _subscribe();
    return () => _unsubscribe();
}, []);
```

We find out that $\alpha \in [-\pi, \pi]$, with $\alpha =0$ when `up` faces east and rotating positively counterclockwise. $\beta \in [-\frac{\pi}2, \frac{\pi}2]$, with $\beta =0$ when `up` is level with the horizon and changing with the obvious up-down directions. Finally, $\gamma \in [-\pi, \pi]$, with $\gamma = \pm 0$ when the camera is facing directly down, getting positive as the camera turns to your right, then reaching $\pm \pi$ when the camera faces the sky.

Now, we will translate these to the `up` and `lookAt` vectors. Note that by default, Three.js sets `camera.up = (0, 1, 0)`, implying that the $y$ axis is the "up-down" axis. Following this convention, let $x$ be the axis pointing east, $y$ be the axis pointing towards the sky, and $z$ be right-hand orthogonal to the $x$ and $y$ axes (thus pointing towards the viewer). We will place our camera by starting with fixed unit vectors, then coming up with ways to use $\alpha, \beta$ and $\gamma$ to transform them into our desired arrangement in space.

Let's imagine that the top of our phone was initially pointing east at the horizon, shooting a unit vector along the $+x$ axis. To achieve our current position, we will rotate this by $\beta$ about the $z$ axis, which will tip the phone $\beta$ radians up or down from the horizon. Then, we rotate by $\alpha$ about the $y$ axis, changing the cardinal direction we are facing. For each rotation we use the corresponding canonical rotation matrix in three dimensions, applying them to our initial vector through matrix multiplication, as follows:

$$
\begin{bmatrix}
    1 \\
    0 \\
    0 \\
\end{bmatrix}
\begin{bmatrix}
    \cos \beta & -\sin \beta  & 0 \\
    \sin \beta & \cos \beta & 0 \\
    0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
    \cos \alpha & 0 & \sin \alpha \\
    0 & 1 & 0 \\
    -\sin \alpha & 0 & \cos \alpha \\
\end{bmatrix} = 
\begin{bmatrix}
    \cos \alpha \cdot \cos \beta \\
    \sin \beta \\
    -\sin \alpha \cdot \cos \beta \\
\end{bmatrix}
$$

This gives us the unit `up` vector; notice this is independent of $\gamma$. You can verify this for yourself by looping `console.log(alpha, beta, gamma)`. If you point the top of your phone towards a given direction, `alpha` and `beta` will remain constant, even as you spin your phone around that axis.

This also gives a key insight for finding the `lookAt` vector; it must be orthogonal to `up`. Hence we only need to check the circular disk normal to `up`, and `lookAt` should lie on this disk. How do we relate this to $\gamma$? It's hard to compute `lookAt` directly, but we can perhaps find `lookAt` for a specific value of $\gamma$, then rotate the result around `up`. 

Recall that $\gamma = \pm 0$ when `lookAt` is pointing in the $-y$ direction; that is, when its $y$ component is maximally negative. Let's call this `lookAt_0`. There are several ways to compute this, but one way is to project the $-y$ axis (or equivalently, the unit vector in that direction) onto `up`'s normal plane. The formula for projecting vector $\vec{u}$ onto a plane defined by normal vector $\vec{n}$ is $\vec{u} - (\vec{u} \cdot \vec{n})/||\vec{n}||^2 \vec{n}$. Luckily, our vectors our already unit vectors, so we can ignore the normalization term and continue as follows:

$$
\begin{align} 
\texttt{lookAt}_0 =& \begin{bmatrix}
    0\\
    -1 \\
    0 \\
    \end{bmatrix} 
    -
    \left(\begin{bmatrix}
    0\\
    -1 \\
    0 \\
    \end{bmatrix} \cdot
    \begin{bmatrix}
    \cos \alpha \cdot \cos \beta \\
    \sin \beta \\
    -\sin \alpha \cdot \cos \beta \\
    \end{bmatrix} \right)
    \begin{bmatrix}
    \cos \alpha \cdot \cos \beta \\
    \sin \beta \\
    -\sin \alpha \cdot \cos \beta \\
    \end{bmatrix}\\
=& \begin{bmatrix}
    0\\
    -1 \\
    0 \\
    \end{bmatrix} 
    + \sin\beta \cdot
    \begin{bmatrix}
    \cos \alpha \cdot \cos \beta \\
    \sin \beta \\
    -\sin \alpha \cdot \cos \beta \\
    \end{bmatrix}\\
=& \begin{bmatrix}
    \cos \alpha \cdot \sin\beta \cdot \cos \beta\\
    \sin^2 \beta -1 \\
    -\sin \alpha \cdot \sin\beta \cdot \cos \beta \\
    \end{bmatrix} 
\end{align} 
$$

and notice we can use our favorite trig identity $\sin^2\beta + \cos^2\beta = 1$, giving 
$$
\begin{bmatrix}
    \cos \alpha \cdot \sin\beta \cdot \cos \beta\\
    -\cos^2 \beta \\
    -\sin \alpha \cdot \sin\beta \cdot \cos \beta \\
    \end{bmatrix}
$$
which conveniently allows us to normalize by dividing away $\cos \beta$: 
$$
\begin{bmatrix}
    \cos \alpha \cdot \sin\beta \\
    -\cos \beta \\
    -\sin \alpha \cdot \sin\beta \\
    \end{bmatrix}.
$$

All that is left is to rotate this about `up` by $\gamma$. There is a [formula](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula) for vector-vector rotation, but Three also gives us an efficient way to do this rotation without object creation in `applyAxisAngle`, and we will use this in our implementation.

So all this math gives us just about 10 lines of code, giving us this relatively lightweight frame-by-frame solution: 

```ts
const up = new THREE.Vector3(
    Math.cos(alpha) * Math.cos(beta),
    Math.sin(beta),
   -Math.sin(alpha) * Math.cos(beta),
);
const lookAt_0 = new THREE.Vector3(
    Math.cos(alpha) * Math.sin(beta),
   -Math.cos(beta),
   -Math.sin(alpha) * Math.sin(beta),
);

camera.up.set(up.x, up.y, up.z);
camera.lookAt(lookAt_0.applyAxisAngle(up, gamma));
```

Hence we've set up a camera in React Three Fiber that can be fully controlled through device motion. Once you build a scene around it, assuming the horizon to be parallel to the $xz$ plane and $+y$ to point towards the sky, you've created a full AR environment.

AR/VR seems to be the next holy grail in the big tech race. However, the computer vision algorithms powering production-grade AR are still computationally expensive, so it remains relatively unexplored in open source and smaller projects. Nonetheless, whether to prepare for forwards compatibility in a possible paradigm shift or to keep up with the UI/UX zeitgeist, developers may want a lightweight AR solution like this in the meantime.

[^1]: This code works for a statically positioned camera, meaning that we only track its rotation in space. Controlling camera position with device motion is significantly more complicated; current accelerometers are wildly inaccurate, which will cause significant noise and unwanted positional drift. Therefore, state-of-the-art AR systems rely on expensive computer vision algorithms to position the scene geometry on top of real space.
[^2]: To understand why we need this, look at an object near you. Then, continue looking at this object, but tilt your head 90 degrees to the left. You are still looking in the same direction, but your view of it has changed.
[^3]: You can find the documentation [here](https://docs.expo.dev/versions/latest/sdk/devicemotion/#devicemotionmeasurement). Although these look like the [Euler angles](https://en.wikipedia.org/wiki/Euler_angles), they are not.